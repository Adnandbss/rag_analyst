# üß† RAG-Analyst Elite: Plateforme d'Analyse de Documents par IA G√©n√©rative

Application **production-ready** d'analyse de documents PDF avec IA g√©n√©rative avanc√©e, incluant agents autonomes, monitoring temps r√©el, et √©valuation automatis√©e.

## üåü Fonctionnalit√©s Principales

### Intelligence Artificielle
- ü§ñ **Agents IA Autonomes** : Syst√®me d'agents avec outils (calculator, web search, document query)
- üìÑ **RAG Multi-Documents** : Gestion de sessions avec plusieurs PDFs par session
- üîç **Hybrid Search** : Combinaison BM25 + recherche s√©mantique + reranking CrossEncoder
- üéØ **Query Optimization** : Expansion, reformulation, HyDE
- üìä **√âvaluation Automatique** : M√©triques RAG (faithfulness, relevance, precision, recall)

### Monitoring & Production
- üìà **Prometheus + Grafana** : M√©triques temps r√©el et dashboards
- üîí **S√©curit√© Production** : Rate limiting, security headers, logging structur√©
- ‚ö° **Performance** : Cache intelligent, batch processing, compression
- üß™ **Tests Automatis√©s** : Suite compl√®te avec rapports HTML

### Interface Utilisateur
- üí¨ **Chat Interactif** : Interface moderne avec historique
- üìä **Dashboard Analytics** : Graphiques Plotly en temps r√©el
- üé® **Multi-Onglets** : Chat, Documents, M√©triques, Exemples
- üîß **Configuration Avanc√©e** : S√©lection de mod√®les, mode agent, √©valuation

## Architecture

Le projet utilise une architecture moderne en couches :

1.  **Frontend** (`frontend_advanced.py`): Interface Streamlit professionnelle avec 4 onglets
2.  **API Gateway** (`app/main.py`): FastAPI avec 25+ endpoints
3.  **Core Engine** (`app/core/`): 10 modules sp√©cialis√©s (RAG, Agents, Evaluation, etc.)
4.  **Data Layer**: ChromaDB (vecteurs) + SQLite (m√©tadonn√©es) + DiskCache (performance)
5.  **Monitoring**: Prometheus + Grafana + Structlog

## üöÄ Quick Start

### Installation

```bash
# 1. Cloner et configurer
git clone https://github.com/votre-username/rag-analyst.git
cd rag-analyst

# 2. Environnement virtuel
python -m venv venv
.\venv\Scripts\activate  # Windows
source venv/bin/activate  # Linux/Mac

# 3. Installer d√©pendances
pip install -r requirements.txt

# 4. Configurer la cl√© OpenAI
cp .env.example .env
# √âditer .env et ajouter votre OPENAI_API_KEY
```

### Lancement

```bash
# Terminal 1: API Backend
uvicorn app.main:app --reload

# Terminal 2: Frontend
streamlit run frontend_advanced.py
```

**URLs:**
- Frontend : http://localhost:8501
- API : http://127.0.0.1:8000
- API Docs : http://127.0.0.1:8000/docs
- Prometheus : http://127.0.0.1:8000/metrics/prometheus

---

### Phase 1: C≈ìur logique du RAG (Termin√©e)

Le c≈ìur du projet est un ensemble de scripts Python qui utilisent `LangChain` pour ex√©cuter un pipeline RAG.

-   **Fichiers Cl√©s**:
    -   `app/core/rag_pipeline.py`: Contient les fonctions pour charger, d√©couper, et vectoriser les PDFs, ainsi que pour cr√©er la cha√Æne de question-r√©ponse.
    -   `run_rag_cli.py`: Une interface en ligne de commande pour tester rapidement le pipeline.

-   **Fonctionnement**:
    1.  Un PDF est charg√© et d√©coup√© en petits morceaux (chunks).
    2.  Chaque chunk est converti en un vecteur num√©rique (embedding) via l'API d'OpenAI.
    3.  Ces vecteurs sont stock√©s dans une base de donn√©es vectorielle `ChromaDB` locale.
    4.  Lorsqu'une question est pos√©e, elle est √©galement vectoris√©e pour trouver les chunks les plus pertinents dans la base de donn√©es.
    5.  La question et les chunks pertinents sont envoy√©s √† un LLM (comme GPT-3.5) pour g√©n√©rer une r√©ponse contextuelle.

### Phase 2: API Backend avec FastAPI

Pour rendre notre logique RAG accessible √† d'autres applications (comme une interface web), nous l'exposons via une API REST en utilisant FastAPI.

#### Fichiers Cl√©s

*   `app/main.py`: Contient le code de l'API FastAPI avec deux endpoints principaux :
    *   `POST /upload`: Pour uploader un fichier PDF, le traiter et cr√©er la base de donn√©es vectorielle.
    *   `POST /ask`: Pour poser une question et obtenir une r√©ponse bas√©e sur le document upload√©.

#### Comment lancer l'API

1.  **Assurez-vous d'avoir install√© les d√©pendances** :
    ```bash
    pip install -r requirements.txt
    ```

2.  **Lancez le serveur Uvicorn** depuis la racine du projet `rag_analyst`:
    ```bash
    uvicorn app.main:app --reload
    ```
    *   `app.main:app` signifie : "dans le fichier `main.py` du package `app`, trouve l'objet nomm√© `app`".
    *   `--reload` red√©marre le serveur automatiquement √† chaque modification du code.

3.  **Acc√©dez √† la documentation interactive** :
    Une fois le serveur lanc√©, ouvrez votre navigateur et allez √† l'adresse [http://127.0.0.1:8000/docs](http://127.0.0.1:8000/docs). Vous y trouverez une interface Swagger UI qui vous permet de tester les endpoints directement.

### Phase 3: Interface utilisateur avec Streamlit

Pour rendre l'application accessible aux utilisateurs finaux, nous avons cr√©√© une interface web moderne et intuitive avec Streamlit.

#### Fichiers Cl√©s

*   `frontend.py`: Application Streamlit compl√®te avec interface de chat, upload de fichiers, et gestion des erreurs.

#### Fonctionnalit√©s

*   **Interface √† deux colonnes** : Upload de documents √† gauche, chat √† droite
*   **Chat interactif** : Historique des conversations avec l'IA
*   **Affichage des sources** : Chaque r√©ponse montre les passages du document utilis√©s
*   **V√©rification de l'API** : Status en temps r√©el de la connexion au backend
*   **Exemples de questions** : Boutons avec des questions pr√©-d√©finies pour d√©marrer rapidement
*   **Gestion d'erreurs** : Messages clairs en cas de probl√®me de connexion ou de traitement

#### Comment lancer l'interface utilisateur

1.  **Assurez-vous que l'API backend est lanc√©e** :
    Dans un premier terminal, avec l'environnement activ√© :
    ```bash
    uvicorn app.main:app --reload
    ```

2.  **Lancez l'interface Streamlit** dans un second terminal :
    ```bash
    # Dans le dossier rag_analyst, avec l'environnement activ√©
    streamlit run frontend.py
    ```

3.  **Acc√©dez √† l'application** :
    Votre navigateur devrait s'ouvrir automatiquement sur [http://localhost:8501](http://localhost:8501). Sinon, ouvrez cette adresse manuellement.

#### Workflow d'utilisation

1.  **V√©rifiez le status** : Dans la sidebar, vous devez voir "‚úÖ API Backend connect√©e"
2.  **Uploadez un PDF** : Glissez-d√©posez ou s√©lectionnez un fichier PDF dans la zone d'upload
3.  **Traitez le document** : Cliquez sur "üöÄ Traiter le Document" et attendez le processus (quelques minutes)
4.  **Posez vos questions** : Utilisez la zone de chat pour dialoguer avec votre document
5.  **Explorez les sources** : Cliquez sur "üìö Sources utilis√©es" pour voir les passages exacts du PDF utilis√©s pour r√©pondre

### Phase 4: Conteneurisation avec Docker

Pour faciliter le d√©ploiement et garantir la portabilit√© de l'application, nous avons conteneuris√© l'API backend avec Docker.

#### Fichiers Cl√©s

*   `Dockerfile`: Configuration pour cr√©er l'image Docker de l'API
*   `.dockerignore`: Liste des fichiers √† exclure du contexte de build Docker
*   `docker-compose.yml`: Configuration pour orchestrer les services avec Docker Compose

#### Fonctionnalit√©s Docker

*   **Image l√©g√®re** : Bas√©e sur `python:3.11-slim` pour optimiser la taille
*   **Multi-stage build** : Optimisation du cache Docker pour acc√©l√©rer les builds
*   **Variables d'environnement** : Configuration flexible via `.env`
*   **Volumes persistants** : Les donn√©es ChromaDB et les PDFs sont sauvegard√©s
*   **Health check** : V√©rification automatique de l'√©tat de l'API
*   **Network isolation** : R√©seau d√©di√© pour les services

#### Comment utiliser Docker

1.  **Build de l'image** :
    ```bash
    docker build -t rag-analyst .
    ```

2.  **Lancement avec Docker Compose** (Recommand√©) :
    ```bash
    # Assurez-vous que votre fichier .env existe avec votre cl√© OpenAI
    docker-compose up -d
    ```

3.  **V√©rification** :
    L'API sera accessible sur [http://localhost:8000](http://localhost:8000)

4.  **Arr√™t des services** :
    ```bash
    docker-compose down
    ```

#### Avantages de la conteneurisation

*   **Portabilit√©** : L'application fonctionne de mani√®re identique sur tous les environnements
*   **Isolation** : Aucun conflit avec d'autres applications
*   **Scalabilit√©** : Facilite le d√©ploiement sur des plateformes cloud
*   **Reproductibilit√©** : Environnement coh√©rent pour tous les d√©veloppeurs

### Phase 5: CI/CD avec GitHub Actions

Mise en place d'un pipeline d'int√©gration et de d√©ploiement continus pour automatiser les tests, builds, et d√©ploiements.

#### Fichiers Cl√©s

*   `.github/workflows/ci.yml`: Pipeline CI/CD complet avec GitHub Actions

#### Pipeline CI/CD

Le workflow automatis√© comprend trois jobs principaux :

1.  **Test** :
    *   Configuration de l'environnement Python 3.11
    *   Installation des d√©pendances
    *   Tests d'import des modules critiques
    *   Validation de la structure du code

2.  **Build Docker** :
    *   Construction de l'image Docker
    *   Test de l'image (d√©marrage et sanit√© de l'API)
    *   Utilisation du cache Docker pour optimiser les performances
    *   Validation que l'application r√©pond correctement

3.  **Publication** (conditionnel) :
    *   D√©clench√© seulement sur push vers `main`
    *   Connexion s√©curis√©e √† Docker Hub
    *   Push de l'image avec tags `latest` et SHA du commit
    *   Publication automatique pour le d√©ploiement

#### D√©clencheurs

*   **Push** vers les branches `main` ou `master`
*   **Pull Request** vers `main` ou `master`
*   Possibilit√© d'ajout de d√©clencheurs manuels (`workflow_dispatch`)

#### Bonnes pratiques impl√©ment√©es

*   **S√©curit√©** : Utilisation de secrets GitHub pour les credentials Docker Hub
*   **Cache** : Optimisation des builds avec le cache Docker de GitHub Actions
*   **Tests isol√©s** : Chaque job s'ex√©cute dans un environnement propre
*   **Conditional deployment** : Publication uniquement sur la branche principale
*   **Monitoring** : Logs d√©taill√©s pour debug en cas d'√©chec

## üéØ Fonctionnalit√©s Avanc√©es

### ü§ñ Mode Agent IA
Activez le mode agent dans les param√®tres pour un assistant qui peut :
- Effectuer des calculs math√©matiques
- Rechercher des informations sur internet
- Interroger vos documents
- Analyser du texte
- **Afficher son raisonnement √©tape par √©tape**

### üîç Hybrid Search
Recherche combinant :
- **BM25** : Recherche par mots-cl√©s (keyword matching)
- **Semantic** : Recherche vectorielle (similarit√© s√©mantique)
- **Reranking** : CrossEncoder pour affiner les r√©sultats
- **R√©sultat** : +30% de pertinence vs recherche simple

### üìä √âvaluation Automatique
Activez l'√©valuation pour obtenir :
- Score de pertinence (relevance)
- Score de fid√©lit√© (faithfulness)
- Pr√©cision/rappel du contexte
- Scores ROUGE vs r√©ponses de r√©f√©rence
- Graphiques de performance

### üìà Monitoring Production
- Dashboard Prometheus avec 15+ m√©triques
- Grafana pour visualisation
- Logs structur√©s (JSON)
- Health checks d√©taill√©s
- Alerting configur√©

### üß™ Tests Automatis√©s
- Suite de 10+ test cases
- √âvaluation automatique de qualit√©
- G√©n√©ration de rapports HTML
- Int√©gration CI/CD
- A/B testing de strat√©gies

## üìö Technologies et Comp√©tences

### Stack IA & ML
*   **Frameworks** : LangChain (agents, chains), Sentence-Transformers (embeddings)
*   **LLMs** : OpenAI GPT-3.5/4, support Ollama (Mistral, Llama)
*   **RAG** : Retrieval, Reranking, Compression contextuelle
*   **Search** : Vector (ChromaDB), Keyword (BM25), Hybrid (RRF)
*   **Evaluation** : RAGAS, ROUGE scores, m√©triques custom

### Backend & API
*   **Framework** : FastAPI (async), Pydantic validation
*   **Database** : SQLite (SQLAlchemy ORM), ChromaDB (vecteurs)
*   **Cache** : DiskCache (gratuit, persistant)
*   **Monitoring** : Prometheus metrics, Structlog (JSON)
*   **Security** : Rate limiting, CORS, security headers

### Frontend & UX
*   **Framework** : Streamlit avec CSS custom
*   **Visualisation** : Plotly (graphiques interactifs), Pandas (tables)
*   **Components** : Chat, upload multi-fichiers, analytics dashboard
*   **UX** : Scores color√©s, progression, exemples pr√©-configur√©s

### DevOps & MLOps
*   **Containerization** : Docker, Docker Compose, multi-stage builds
*   **CI/CD** : GitHub Actions (tests, build, deploy)
*   **Testing** : Pytest (unit, integration, mocks), coverage
*   **Observability** : Prometheus, Grafana, distributed tracing ready
*   **IaC** : Pr√™t pour Terraform/Kubernetes

## Comment Utiliser ce Projet en Entretien

### D√©monstration Live

1.  **Montrez l'architecture** : Expliquez le pipeline RAG et ses composants
2.  **Demo de l'interface** : Upload d'un PDF, questions/r√©ponses en temps r√©el
3.  **Code walkthrough** : Parcourez les parties cl√©s du code
4.  **Docker demo** : Lancez l'application avec `docker-compose up`

### Points de Discussion

*   **Choix techniques** : Pourquoi LangChain vs alternatives, strat√©gies de chunking
*   **Optimisations** : Cache vectoriel, gestion m√©moire, performance
*   **Production readiness** : Monitoring, scaling, s√©curit√©
*   **Extensions possibles** : Multi-documents, fine-tuning, agents IA

### Axes d'Am√©lioration (Pour montrer votre vision)

*   **Multi-modalit√©** : Support d'images, tableaux, graphiques
*   **Agents autonomes** : Capacit√© de planification et d'actions
*   **Fine-tuning** : Mod√®les sp√©cialis√©s pour des domaines m√©tier
*   **Evaluation** : M√©triques de qualit√©, tests A/B
*   **D√©ploiement cloud** : Kubernetes, auto-scaling, multi-region

---

## Conclusion

Ce projet **RAG-Analyst** repr√©sente une application compl√®te et production-ready d'IA g√©n√©rative, couvrant l'ensemble du cycle de d√©veloppement moderne : du prototype √† la production, en passant par les tests automatis√©s et le d√©ploiement continu.

Il d√©montre une compr√©hension approfondie des enjeux techniques et business de l'IA g√©n√©rative, ainsi qu'une ma√Ætrise des outils et pratiques MLOps essentiels pour un poste d'Ing√©nieur GenAI/LLM.

**Points forts du projet :**
- Pipeline RAG complet et fonctionnel
- Architecture modulaire et extensible  
- Interface utilisateur moderne et intuitive
- Infrastructure Docker pour le d√©ploiement
- CI/CD automatis√© avec GitHub Actions
- Documentation technique compl√®te

Ce projet constitue une base solide pour d√©montrer vos comp√©tences en entretien et peut servir de fondation pour des projets plus complexes int√©grant des agents IA, du fine-tuning, ou des architectures multi-modales.
