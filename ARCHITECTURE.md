# Architecture RAG-Analyst v2.0

## Vue d'Ensemble

RAG-Analyst est une plateforme d'analyse de documents par IA g√©n√©rative, con√ßue avec une architecture moderne, scalable et orient√©e production.

## Architecture Syst√®me (C4 Level 2)

```mermaid
graph TB
    User[üë§ Utilisateur]
    Frontend[üé® Frontend Streamlit]
    API[‚ö° API FastAPI]
    
    subgraph "Core Engine"
        RAG[üß† RAG Pipeline]
        Agent[ü§ñ AI Agents]
        Eval[üìä Evaluation]
        Hybrid[üîç Hybrid Search]
    end
    
    subgraph "Data Layer"
        VectorDB[(üì¶ ChromaDB)]
        SQLite[(üóÑÔ∏è SQLite)]
        Cache[(‚ö° DiskCache)]
    end
    
    subgraph "External Services"
        OpenAI[üåê OpenAI API]
        WebSearch[üåê DuckDuckGo]
    end
    
    subgraph "Monitoring"
        Prometheus[üìä Prometheus]
        Grafana[üìà Grafana]
        Logs[üìù Structlog]
    end
    
    User --> Frontend
    Frontend --> API
    API --> RAG
    API --> Agent
    API --> Eval
    
    RAG --> VectorDB
    RAG --> SQLite
    RAG --> OpenAI
    
    Agent --> WebSearch
    Agent --> RAG
    
    Hybrid --> VectorDB
    RAG --> Hybrid
    
    API --> Prometheus
    Prometheus --> Grafana
    API --> Logs
```

## Flux de Donn√©es

### 1. Upload et Traitement de Document

```mermaid
sequenceDiagram
    participant U as User
    participant F as Frontend
    participant A as API
    participant R as RAG Pipeline
    participant V as VectorDB
    participant D as SQLite
    
    U->>F: Upload PDF
    F->>A: POST /sessions/{id}/upload
    A->>R: process_document()
    R->>R: Extract text + chunk
    R->>V: Store embeddings
    R->>D: Save metadata
    R-->>A: Processing result
    A-->>F: Success + metrics
    F-->>U: Display confirmation
```

### 2. Question avec Agent IA

```mermaid
sequenceDiagram
    participant U as User
    participant A as API
    participant Agent as AI Agent
    participant Tools as Tools
    participant OpenAI as OpenAI
    
    U->>A: POST /ask (use_agent=true)
    A->>Agent: run(question)
    Agent->>OpenAI: Analyze question
    OpenAI-->>Agent: Action plan
    
    loop For each action
        Agent->>Tools: Execute tool
        Tools-->>Agent: Tool result
        Agent->>OpenAI: Process result
    end
    
    Agent-->>A: Final answer + reasoning
    A-->>U: Response + tools used
```

## Composants Principaux

### 1. API Layer (`app/main.py`)
- **FastAPI** avec 25+ endpoints
- **CORS** configur√© pour Streamlit
- **Middlewares** : Security headers, Request logging, Rate limiting
- **Validation** : Pydantic models strictes
- **Endpoints cl√©s** :
  - `/sessions/*` : Gestion des sessions
  - `/ask` : Question-r√©ponse standard
  - `/ask-stream` : Streaming SSE
  - `/evaluation/*` : Tests automatis√©s
  - `/metrics/prometheus` : Monitoring

### 2. RAG Engine (`app/core/`)

#### a) `advanced_rag.py` - Pipeline RAG Avanc√©
- Gestion multi-documents par session
- Chunking intelligent avec m√©tadonn√©es
- Compression contextuelle optionnelle
- Tracking complet des performances

#### b) `agents.py` + `tools.py` - Syst√®me d'Agents
- Pattern **ReAct** (Reasoning + Acting)
- 5 outils int√©gr√©s :
  - Calculator : Calculs math√©matiques
  - Web Search : Recherche internet (DuckDuckGo)
  - DateTime : Date/heure actuelle
  - Document Query : Interrogation des docs
  - Text Analysis : Analyse de texte
- Affichage du raisonnement √©tape par √©tape

#### c) `hybrid_search.py` - Recherche Hybride
- **BM25** pour recherche par mots-cl√©s
- **Semantic Search** pour recherche vectorielle
- **RRF** (Reciprocal Rank Fusion)
- **CrossEncoder** reranking pour affiner

#### d) `query_optimizer.py` - Optimisation de Requ√™tes
- **Query Expansion** : G√©n√©ration de variations
- **HyDE** : Hypothetical Document Embeddings
- **Query Decomposition** : Sous-questions

#### e) `evaluation.py` + `rag_evaluation_suite.py` - √âvaluation
- M√©triques RAG : Relevance, Faithfulness, Precision, Recall
- ROUGE scores pour comparaison
- Suite de tests automatis√©s
- G√©n√©ration de rapports HTML

### 3. Data Layer

#### a) ChromaDB (Vector Store)
- Stockage des embeddings OpenAI
- Un vector store par session (`chroma_db/session_{id}/`)
- Recherche par similarit√© cosine
- Persistance sur disque

#### b) SQLite (Metadata Store)
- Sessions utilisateur
- Documents trait√©s
- Historique des conversations
- M√©triques d'√©valuation
- Schema avec SQLAlchemy ORM

#### c) DiskCache (Performance Cache)
- Cache des embeddings calcul√©s
- Cache des r√©ponses fr√©quentes
- R√©duction des co√ªts API

### 4. Monitoring & Observabilit√©

#### Prometheus Metrics
- Requ√™tes API (count, duration, status)
- Questions RAG (count, duration, scores)
- Documents (processing time, chunks)
- Agents (tool usage, execution time)
- Erreurs (par type, par composant)

#### Structured Logging
- Tous les √©v√©nements loggu√©s avec **structlog**
- Format JSON pour parsing facile
- Corr√©lation par session_id
- Niveaux : INFO, WARNING, ERROR

## D√©cisions d'Architecture

### Choix Techniques

| D√©cision | Choix | Alternatives Consid√©r√©es | Raison |
|----------|-------|-------------------------|---------|
| Framework API | FastAPI | Flask, Django REST | Performance async, validation Pydantic, doc auto |
| Vector DB | ChromaDB | Pinecone, Weaviate, FAISS | Gratuit, local, facile √† setup |
| LLM Framework | LangChain | LlamaIndex, Custom | Ecosyst√®me complet, agents int√©gr√©s |
| Database | SQLite | PostgreSQL, MongoDB | Simplicit√©, pas de serveur requis |
| Frontend | Streamlit | React, Vue | Rapid prototyping, Python natif |
| Monitoring | Prometheus | Datadog, New Relic | Open-source, standard industrie |
| Testing | Pytest | Unittest, Nose | Fixtures puissantes, plugins riches |
| Containerization | Docker | Podman, containerd | Standard de facto, bon ecosystem |

### Trade-offs

#### 1. SQLite vs PostgreSQL
**Choix : SQLite**
- ‚úÖ Avantages : Zero config, portable, gratuit
- ‚ùå Inconv√©nients : Pas de concurrence √©lev√©e, limites de scalabilit√©
- **Quand migrer** : >1000 sessions actives ou besoin de multi-instance

#### 2. ChromaDB Local vs Cloud Vector DB
**Choix : ChromaDB Local**
- ‚úÖ Avantages : Gratuit, privacit√© des donn√©es, latence minimale
- ‚ùå Inconv√©nients : Pas de r√©plication, backup manuel
- **Quand migrer** : Besoin de haute disponibilit√© ou multi-region

#### 3. Synchrone vs Asynchrone
**Choix : Hybrid (FastAPI async + sync core)**
- ‚úÖ FastAPI async pour I/O
- ‚úÖ Core logic synchrone (LangChain)
- Bon compromis simplicit√©/performance

## Patterns & Best Practices

### 1. Separation of Concerns
```
Frontend (UI) ‚Üí API (Business Logic) ‚Üí Core (Domain Logic) ‚Üí Data (Persistence)
```

### 2. Dependency Injection
```python
def endpoint(db: Session = Depends(get_db)):
    # Testable, modulaire
```

### 3. Error Handling
- Exceptions custom par couche
- Logging structur√© syst√©matique
- Fallbacks gracieux

### 4. Configuration
- Variables d'environnement (`.env`)
- Config par mod√®le centralis√©e
- S√©lection adaptative de mod√®les

## Scalabilit√©

### Limitations Actuelles
- Single-instance (pas de load balancing)
- SQLite (limite de concurrence)
- In-memory state (current_pipeline, current_agent)
- Pas de queue pour jobs longs

### Plan de Scaling

#### Phase 1 : Optimisation Verticale
- Redis pour cache distribu√©
- PostgreSQL pour DB
- Connection pooling optimis√©

#### Phase 2 : Scaling Horizontal
- Load balancer (Nginx)
- Multiple API instances
- Shared state avec Redis
- Queue syst√®me (Celery + RabbitMQ)

#### Phase 3 : Cloud Native
- Kubernetes deployment
- Auto-scaling pods
- Distributed tracing
- Multi-region

## S√©curit√©

### Impl√©mentations Actuelles
- ‚úÖ Rate limiting (100 req/heure par IP)
- ‚úÖ Security headers (HSTS, XSS, etc.)
- ‚úÖ Input validation (Pydantic)
- ‚úÖ Request logging complet
- ‚úÖ CORS configur√©

### √Ä Ajouter pour Production
- JWT authentication
- Role-based access control (RBAC)
- API key management
- WAF (Web Application Firewall)
- Chiffrement des donn√©es sensibles
- Audit trail complet

## Performance

### Optimisations Actuelles
- Chunking optimis√© (1000 chars, 200 overlap)
- Batch embedding possible
- Cache disque des embeddings
- Context compression avec LLM

### M√©triques Cibles
- Temps de r√©ponse : <3s (p95)
- Temps de traitement document : <30s/page
- Score de confiance : >0.7 (moyenne)
- Disponibilit√© : >99%

## Monitoring

### Dashboards Grafana
1. **Operational Dashboard**
   - Requ√™tes/minute
   - Temps de r√©ponse (p50, p95, p99)
   - Taux d'erreur
   - Sessions actives

2. **Quality Dashboard**
   - Scores de confiance
   - Scores d'√©valuation RAG
   - Taux de passage des tests

3. **Business Dashboard**
   - Documents trait√©s
   - Questions par cat√©gorie
   - Co√ªts API estim√©s
   - Utilisation des agents

## Co√ªts

### Estimation Mensuelle (1000 questions/mois)
- OpenAI embeddings : ~$0.10
- OpenAI GPT-3.5 : ~$2.00
- Agent avec outils : ~$3.00
- GPT-4 (si utilis√©) : ~$30.00

**Total : $5-35/mois selon usage**

## Roadmap Future

### Court Terme (1-2 mois)
- [ ] Authentification JWT
- [ ] PostgreSQL migration
- [ ] Cache Redis
- [ ] Streaming r√©el (pas simul√©)

### Moyen Terme (3-6 mois)
- [ ] Support multi-langue
- [ ] Fine-tuning de mod√®les custom
- [ ] Multi-modal complet (images, tableaux)
- [ ] Agents avanc√©s avec memory

### Long Terme (6-12 mois)
- [ ] Kubernetes deployment
- [ ] Auto-scaling
- [ ] Multi-tenant SaaS
- [ ] Marketplace d'agents custom

## Diagramme de D√©ploiement

```mermaid
graph LR
    subgraph "Production"
        LB[Load Balancer]
        API1[API Instance 1]
        API2[API Instance 2]
        Redis[(Redis Cache)]
        Postgres[(PostgreSQL)]
        S3[(S3 Storage)]
    end
    
    subgraph "Monitoring"
        Prom[Prometheus]
        Graf[Grafana]
        Alert[AlertManager]
    end
    
    LB --> API1
    LB --> API2
    API1 --> Redis
    API2 --> Redis
    API1 --> Postgres
    API2 --> Postgres
    API1 --> S3
    
    API1 --> Prom
    API2 --> Prom
    Prom --> Graf
    Prom --> Alert
```

## Contact & Contribution

Pour toute question sur l'architecture ou proposition d'am√©lioration, consultez le README.md.

---
*Document maintenu par : √âquipe RAG-Analyst*
*Derni√®re mise √† jour : Octobre 2024*

